{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis\n",
    "\n",
    "A.k.a meaning generation. The process of determining the meaning of character sequences or word sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation       \t-\n",
      "conjunction    \t&\n",
      "disjunction    \t|\n",
      "implication    \t->\n",
      "equivalence    \t<->\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.boolean_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER \n",
    "Named entity recognition is the process in which proper nouns or named entities are located in a document. Then, these Named Entities are classified into different categories, such as Name of Person, Location, Organization and so on.\n",
    "\n",
    "- NEP: Name of Person\n",
    "- NED: Name of Designation\n",
    "- NEO: Name of Organization \n",
    "- NEA: Name of Abbreviation\n",
    "- NEB: Name of Brand\n",
    "- NETP: Title of Person\n",
    "- NETO: Title of Object\n",
    "- NEL: Name of Location\n",
    "- NETI: Time\n",
    "- NEN: Number\n",
    "- NEM: Measure\n",
    "- NETE: Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download it from:\n",
    "# https://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "sentence = StanfordNERTagger('./stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                            './stanford-ner-2018-10-16/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'PERSON'), ('goes', 'O'), ('to', 'O'), ('NY', 'O')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing NER using Stanford Tagger\n",
    "sentence.tag(word_tokenize('John goes to NY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/alextanhongpin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/alextanhongpin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "sentence = nltk.corpus.treebank.tagged_sents()[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/var/folders/pt/v3mw_j891dv8yl5k2b5p54wm0000gn/T/tmpxlf_09_n.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/ipython/7.5.0/libexec/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m                 ).split()\n\u001b[1;32m    803\u001b[0m             )\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/var/folders/pt/v3mw_j891dv8yl5k2b5p54wm0000gn/T/tmpxlf_09_n.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('The', 'DT'), ('total', 'NN'), ('of', 'IN'), ('18', 'CD'), ('deaths', 'NNS'), ('from', 'IN'), ('malignant', 'JJ'), ('mesothelioma', 'NN'), (',', ','), ('lung', 'NN'), ('cancer', 'NN'), ('and', 'CC'), ('asbestosis', 'NN'), ('was', 'VBD'), ('far', 'RB'), ('higher', 'JJR'), ('than', 'IN'), ('*', '-NONE-'), ('expected', 'VBN'), ('*?*', '-NONE-'), (',', ','), ('the', 'DT'), ('researchers', 'NNS'), ('said', 'VBD'), ('0', '-NONE-'), ('*T*-1', '-NONE-'), ('.', '.')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(sentence, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/var/folders/pt/v3mw_j891dv8yl5k2b5p54wm0000gn/T/tmp1fjw606l.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/ipython/7.5.0/libexec/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m                 ).split()\n\u001b[1;32m    803\u001b[0m             )\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/var/folders/pt/v3mw_j891dv8yl5k2b5p54wm0000gn/T/tmp1fjw606l.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('The', 'DT'), ('total', 'NN'), ('of', 'IN'), ('18', 'CD'), ('deaths', 'NNS'), ('from', 'IN'), ('malignant', 'JJ'), ('mesothelioma', 'NN'), (',', ','), ('lung', 'NN'), ('cancer', 'NN'), ('and', 'CC'), ('asbestosis', 'NN'), ('was', 'VBD'), ('far', 'RB'), ('higher', 'JJR'), ('than', 'IN'), ('*', '-NONE-'), ('expected', 'VBN'), ('*?*', '-NONE-'), (',', ','), ('the', 'DT'), ('researchers', 'NNS'), ('said', 'VBD'), ('0', '-NONE-'), ('*T*-1', '-NONE-'), ('.', '.')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER System using Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NER using Machine Learning Toolkits\n",
    "\n",
    "- Rule-based or Handcrafted approach\n",
    "    - list lookup approach\n",
    "    - linguistic approach\n",
    "- Machine-learning based or automated approach\n",
    "    - hidden markov model\n",
    "    - maximum entropy markov model\n",
    "    - conditional random fields \n",
    "    - support vector machine \n",
    "    - decision trees\n",
    "    \n",
    "ML based approaches outperforms the rule-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Smith', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('NY', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Germany', 'NNP')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import pos_tag, word_tokenize\n",
    "pos_tag(word_tokenize('John and Smith are going to NY and Germany'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John -> NP\n",
      "and -> CC\n",
      "Smith -> NP\n",
      "went -> VBD\n",
      "to -> TO\n",
      "NY -> None\n",
      "and -> CC\n",
      "Germany -> NP-TL\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "tagger = UnigramTagger(brown.tagged_sents(categories='news'))\n",
    "sentence = ['John', 'and', 'Smith', 'went', 'to', 'NY', 'and', 'Germany']\n",
    "for word, tag in tagger.tag(sentence):\n",
    "    print(word, '->', tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
