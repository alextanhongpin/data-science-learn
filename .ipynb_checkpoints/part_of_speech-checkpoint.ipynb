{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alextanhongpin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'It is a pleasant day today'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'is', 'a', 'pleasant', 'day', 'today']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = word_tokenize(text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pleasant', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('today', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/alextanhongpin/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VB.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('bear', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('pain', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('bear', 'NN')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word disambiguation \"bear\". \n",
    "# The first \"bear\" is a verb.\n",
    "# The second \"bear\" is a noun.\n",
    "pos_tag(word_tokenize('I cannot bear the pain of bear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## str2tuple and tuple2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import str2tuple, tuple2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bear', 'NN')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str2tuple('bear/NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bear/NN'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple2str(('bear', 'NN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common tags in Treebank corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/alextanhongpin/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/alextanhongpin/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk import bigrams, FreqDist\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 28867),\n",
       " ('VERB', 13564),\n",
       " ('.', 11715),\n",
       " ('ADP', 9857),\n",
       " ('DET', 8725),\n",
       " ('X', 6613),\n",
       " ('ADJ', 6397),\n",
       " ('NUM', 3546),\n",
       " ('PRT', 3219),\n",
       " ('ADV', 3171),\n",
       " ('PRON', 2737),\n",
       " ('CONJ', 2265)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_tagged = treebank.tagged_words(tagset='universal')\n",
    "tag = FreqDist(tag for (word, tag) in treebank_tagged)\n",
    "tag.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Pierre', 'NOUN'), ('Vinken', 'NOUN'))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of tags occuring before a noun tag.\n",
    "tagpairs = bigrams(treebank_tagged)\n",
    "\n",
    "# The result is a bigram tuple, with the type as the second parameter.\n",
    "# ((first_word, type), (second word, type))\n",
    "print(next(tagpairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all bigrams where the second word is a noun.\n",
    "preceders_noun = [x[1] for (x, y) in tagpairs if y[1] == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist = FreqDist(preceders_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " '.',\n",
       " 'VERB',\n",
       " 'NUM',\n",
       " 'PRT',\n",
       " 'CONJ',\n",
       " 'PRON',\n",
       " 'X',\n",
       " 'ADV']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tag for (tag, _) in freqdist.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 7627),\n",
       " ('DET', 5569),\n",
       " ('ADJ', 4474),\n",
       " ('ADP', 3175),\n",
       " ('.', 2599),\n",
       " ('VERB', 1497),\n",
       " ('NUM', 1252),\n",
       " ('PRT', 796),\n",
       " ('CONJ', 792),\n",
       " ('PRON', 573),\n",
       " ('X', 410),\n",
       " ('ADV', 101)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqdist.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = DefaultTagger('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beautiful', 'NN'), ('morning', 'NN')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(['Beautiful', 'morning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beautiful', 'morning']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Untagging a sentence\n",
    "from nltk.tag import untag\n",
    "untag([('Beautiful', 'NN'), ('morning', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "# DEPRECATED: from nltk.tag import FastBrillTaggerTrainer\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag.brill_trainer import BrillTaggerTrainer\n",
    "from nltk.tag.brill import Word, fntbl37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBL train (fast) (seqs: 3914; tokens: 100676; tpls: 37; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 96798 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      " 202 208   6   3  | IN->WDT if Word:that@[0] & Pos:-NONE-@[1]\n",
      " 142 147   5   0  | NN->VB if Pos:TO@[-1] & Pos:-NONE-@[-2]\n",
      "  89  91   2   0  | VBP->VB if Pos:MD@[-3,-2,-1]\n",
      "  76  76   0   0  | VBP->VB if Word:to@[-1]\n",
      "  75  75   0   0  | NN->VB if Pos:MD@[-1]\n",
      "  64  72   8   0  | VBD->VBN if Pos:VBZ@[-2,-1]\n",
      "  58  58   0   1  | POS->VBZ if Pos:PRP@[-2,-1]\n",
      "  58  61   3   0  | VBD->VBN if Pos:VBP@[-2,-1]\n",
      "  57  64   7   0  | VBD->VBN if Pos:VBD@[-2,-1]\n",
      "  55  55   0   3  | VB->NN if Pos:DT@[-1]\n",
      "  47  64  17   3  | VB->VBP if Pos:NNS@[-1]\n",
      "  45  53   8   2  | VBN->VBD if Pos:NNP@[-1]\n",
      "  38  43   5   0  | VBN->VBD if Pos:PRP@[-1]\n",
      "  38  46   8  19  | RP->RB if Pos:CD@[1,2,3]\n",
      "  37  45   8   2  | VB->VBP if Pos:PRP@[-1]\n",
      "  35  35   0   0  | VBP->VB if Word:n't@[-2,-1]\n",
      "  34  39   5   0  | JJR->RBR if Word:more@[0] & Pos:JJ@[1]\n",
      "  31  32   1   0  | IN->RB if Word:as@[0] & Word:as@[2]\n",
      "  31  43  12   1  | VBD->VBN if Pos:VB@[-2,-1]\n",
      "  26  31   5   5  | VB->NN if Pos:JJ@[-2,-1]\n",
      "  23  25   2   1  | IN->DT if Word:that@[0] & Pos:IN@[-1]\n",
      "  21  25   4   0  | NN->VB if Word:n't@[-1]\n",
      "  19  19   0   0  | VB->VBP if Pos:-NONE-@[-1] & Pos:WDT@[-2]\n",
      "  18  18   0   0  | VBN->VBD if Pos:WDT@[-2]\n",
      "  17  22   5   0  | JJS->RBS if Word:most@[0] & Pos:JJ@[1]\n",
      "  17  17   0   0  | POS->VBZ if Word:'s@[0] & Pos:EX@[-1]\n",
      "  15  15   0   0  | VBN->VBD if Pos:NN@[-1] & Pos:DT@[1]\n",
      "  15  18   3   8  | NN->VBP if Pos:PRP@[-1]\n",
      "  14  14   0   0  | VB->VBP if Word:who@[-2,-1]\n",
      "  13  13   0   0  | RP->IN if Word:out@[0] & Word:of@[1]\n",
      "  13  13   0   0  | JJR->RBR if Word:more@[0] & Pos:RB@[1]\n",
      "  13  18   5   1  | JJ->VB if Pos:TO@[-1] & Pos:-NONE-@[-2]\n",
      "  13  14   1   1  | VB->NN if Pos:IN@[-1]\n",
      "  13  17   4   0  | VBD->VBN if Pos:-NONE-@[1] & Pos:IN@[2] & Word:*@[1]\n",
      "  12  14   2   0  | NNP->JJ if Word:American@[0] & Pos:NN@[1]\n",
      "  12  13   1   0  | POS->VBZ if Word:'s@[0] & Pos:-NONE-@[-1]\n",
      "  11  11   0   0  | VBN->VBD if Pos:NNS@[-1] & Pos:DT@[1]\n",
      "  11  11   0   0  | NNS->VBZ if Pos:-NONE-@[-1] & Pos:WDT@[-2]\n",
      "  10  10   0   0  | NNP->JJ if Word:American@[0] & Pos:NNS@[1]\n",
      "  10  10   0   1  | VBN->VBD if Pos:RB@[-1] & Pos:PRP@[-2]\n",
      "   9   9   0   0  | VB->VBN if Word:has@[-2,-1]\n",
      "   9   9   0   0  | VBN->VBD if Word:who@[-2,-1]\n",
      "   9  12   3   0  | DT->PDT if Word:all@[0] & Pos:DT@[1]\n",
      "   9   9   0   0  | NN->VB if Pos:RB@[-1] & Pos:MD@[-2]\n",
      "   8   8   0   0  | JJ->NNP if Word:Soviet@[0] & Word:Union@[1]\n",
      "   8   8   0   3  | IN->VB if Word:like@[0] & Word:to@[2]\n",
      "   8   9   1   0  | NN->JJ if Word:executive@[0] & Pos:NNP@[-2]\n",
      "   8   9   1   0  | DT->RB if Word:no@[0] & Pos:JJR@[1]\n",
      "   8   8   0   0  | JJR->RBR if Word:less@[0] & Pos:JJ@[1]\n",
      "   8  15   7   0  | RB->JJ if Word:much@[0] & Pos:IN@[1]\n",
      "   8   8   0   0  | RB->JJ if Word:much@[0] & Pos:NN@[1]\n",
      "   8   9   1   0  | NN->VBG if Pos:-NONE-@[-1] & Pos:IN@[-2]\n",
      "   8   8   0   0  | NN->VBP if Pos:-NONE-@[-1] & Pos:WDT@[-2]\n",
      "   8   8   0   0  | VBN->VB if Pos:MD@[-1]\n",
      "   7   7   0   0  | JJ->RB if Word:early@[0] & Pos:IN@[1]\n",
      "   7   7   0   1  | VBD->VBN if Pos:DT@[-1] & Pos:NN@[1]\n",
      "   7   8   1   0  | VBN->VBD if Pos:,@[-1] & Pos:DT@[1]\n",
      "   7   7   0   0  | VB->NN if Pos:POS@[-2,-1]\n",
      "   7   7   0   2  | VBP->VB if Pos:VBD@[-2,-1]\n",
      "   6   6   0   0  | JJ->NN if Word:total@[0] & Word:of@[1]\n",
      "   6   6   0   4  | VBP->VBD if Word:it@[-2,-1]\n",
      "   6   6   0   0  | RB->JJ if Word:only@[0] & Pos:NN@[1]\n",
      "   6   6   0   1  | JJ->NN if Word:'s@[1]\n",
      "   6   6   0   0  | VBP->NN if Word:the@[-1]\n",
      "   6   6   0   0  | NN->VBP if Pos:NNS@[-1] & Pos:DT@[1]\n",
      "   6   7   1   1  | VBN->VBD if Pos:CC@[-1] & Pos:DT@[1]\n",
      "   6   8   2   0  | NNS->VBZ if Pos:PRP@[-1]\n",
      "   5   5   0   0  | IN->DT if Word:that@[0] & Word:0@[-1]\n",
      "   5   7   2   0  | JJ->NN if Word:future@[0] & Word:the@[-1]\n",
      "   5  10   5   0  | IN->RB if Word:ago@[0] & Word:,@[1]\n",
      "   5   5   0   0  | JJ->NN if Word:average@[0] & Word:of@[1]\n",
      "   5   6   1   0  | RB->IN if Word:so@[0] & Word:that@[1]\n",
      "   5   5   0   0  | VB->VBN if Word:be@[-2,-1]\n",
      "   5   6   1   0  | JJR->RBR if Word:*-1@[1,2,3]\n",
      "   5   5   0   0  | JJ->RB if Word:first@[0] & Pos:VBD@[1]\n",
      "   5   6   1   0  | JJS->RBS if Word:most@[0] & Pos:RB@[1]\n",
      "   5   5   0   0  | IN->DT if Word:that@[0] & Pos:TO@[-1]\n",
      "   5   7   2   0  | NN->VBG if Word:operating@[0] & Pos:NN@[-1]\n",
      "   5   5   0   0  | POS->VBZ if Word:'s@[0] & Pos:DT@[-1]\n",
      "   5   8   3   0  | PRP$->PRP if Word:her@[0] & Pos:VBD@[-1]\n",
      "   5   7   2   0  | VBZ->NNS if Word:plans@[0] & Pos:NN@[-1]\n",
      "   5   5   0   0  | VBZ->NNS if Word:plans@[0] & Word:*@[1] & Word:to@[2]\n",
      "   5   6   1   1  | VBN->VBD if Word:*T*-1@[-1]\n",
      "   5   5   0   0  | VBG->NN if Pos:JJ@[-1] & Pos:IN@[1]\n",
      "   5   5   0   0  | CD->LS if Pos:.@[1] & Pos:-NONE-@[2]\n",
      "   5   5   0   0  | NN->VBP if Pos:-NONE-@[-1] & Pos:WP@[-2]\n",
      "   5   7   2   0  | VB->VBP if Pos:RB@[-1] & Pos:NNS@[-2]\n",
      "   5   5   0   0  | VBD->VBN if Pos:-NONE-@[-1] & Pos:WRB@[-2]\n",
      "   5  10   5   5  | JJ->NN if Pos:VBD@[1]\n",
      "   5   6   1   2  | JJ->VB if Pos:MD@[-1]\n",
      "   5   6   1   0  | VB->NN if Pos:PRP$@[-1]\n",
      "   5   5   0   0  | VBP->VB if Pos:VBP@[-2]\n",
      "   5   5   0   0  | NNP->JJ if Pos:JJ@[1] & Pos:NNS@[2] & Word:Asian@[1]\n",
      "   4   4   0   0  | VBG->NN if Word:purchasing@[0] & Word:managers@[1] &\n",
      "                  |   Word:'@[2]\n",
      "   4   4   0   1  | JJ->VB if Word:own@[0] & Word:already@[-1]\n",
      "   4   4   0   0  | EX->RB if Word:there@[0] & Word:,@[1]\n",
      "   4   4   0   0  | JJ->NN if Word:commercial@[0] & Word:,@[1]\n",
      "   4   4   0   0  | JJ->RB if Word:early@[0] & Word:this@[1]\n",
      "   4   4   0   0  | NN->JJ if Word:executive@[0] & Word:branch@[1]\n",
      "   4   4   0   0  | POS->'' if Word:'@[0] & Word:''@[1]\n",
      "   4   6   2   0  | RB->IN if Word:up@[0] & Word:to@[1]\n",
      "   4   4   0   0  | VBN->VBD if Word:estimated@[0] & Word:that@[1]\n",
      "   4   4   0   0  | WDT->IN if Word:that@[0] & Word:*@[1]\n",
      "   4   4   0   0  | IN->RB if Word:as@[0] & Word:a@[2]\n",
      "   4   5   1   0  | NNPS->NNP if Word:Securities@[0] & Word:,@[2]\n",
      "   4   4   0   0  | VB->VBN if Word:have@[-2,-1]\n",
      "   4   4   0   0  | VBP->VB if Word:does@[-2,-1]\n",
      "   4   4   0   0  | IN->RB if Word:So@[0] & Pos:-NONE-@[2]\n",
      "   4   5   1   0  | JJR->RBR if Word:more@[0] & Pos:MD@[-2]\n",
      "   4   8   4   0  | IN->RB if Word:as@[0] & Pos:RB@[1]\n",
      "   4   4   0   0  | RB->IN if Word:as@[0] & Word:,@[2]\n",
      "   4   5   1   0  | JJ->RB if Word:long@[0] & Pos:IN@[1]\n",
      "   4   4   0   0  | RB->IN if Word:so@[0] & Pos:PRP@[1]\n",
      "   4   4   0   0  | RB->JJ if Word:enough@[0] & Pos:NNS@[1]\n",
      "   4   4   0   1  | NN->VBP if Word:rise@[0] & Pos:NNS@[-1]\n",
      "   4   4   0   0  | RBS->JJS if Word:most@[0] & Pos:IN@[-1]\n",
      "   4   4   0   0  | VB->VBD if Word:cut@[0] & Pos:NNP@[-1]\n",
      "   4   4   0   0  | VBN->VBD if Word:increased@[0] & Pos:NN@[-1]\n",
      "   4   4   0   0  | VBN->VBD if Word:increased@[0] & Pos:NNS@[-1]\n",
      "   4   6   2   0  | VBZ->NNS if Word:of@[1]\n",
      "   4   4   0   1  | NN->VBP if Word:traders@[-1]\n",
      "   4   4   0   0  | CD->LS if Pos::@[-1] & Pos:-RRB-@[1]\n",
      "   4   4   0   2  | NN->VB if Pos:CC@[-1] & Pos:DT@[1]\n",
      "   4   4   0   0  | NN->VB if Pos:TO@[-1] & Pos:DT@[1]\n",
      "   4   4   0   0  | NNP->JJ if Pos:``@[-1] & Pos:NN@[1]\n",
      "   4   7   3   0  | VBG->NN if Pos:DT@[-1] & Pos:IN@[1]\n",
      "   4   4   0   0  | VBN->VBD if Pos:NN@[-1] & Pos:NNP@[1]\n",
      "   4   4   0   0  | VBN->VBD if Pos:NNS@[-1] & Pos:JJ@[1]\n",
      "   4   4   0   0  | VBN->VBD if Pos:NNS@[-1] & Pos:NNS@[1]\n",
      "   4   5   1   1  | NN->VBP if Pos:RB@[-1] & Pos:NNS@[-2]\n",
      "   4   4   0   0  | NNS->VBZ if Pos:-NONE-@[-1] & Pos:WP@[-2]\n",
      "   4   6   2   0  | VB->VBP if Pos:RB@[-1] & Pos:PRP@[-2]\n",
      "   4   5   1   0  | VBN->VB if Pos:TO@[-1] & Pos:-NONE-@[-2]\n",
      "   4   4   0   0  | VBN->VBD if Pos:RB@[-1] & Pos:NNP@[-2]\n",
      "   4  10   6   2  | RP->IN if Pos:.@[1,2]\n",
      "   3   3   0   0  | NNP->NN if Word:Test@[0] & Word:of@[1] & Word:Basic@[2]\n",
      "   3   3   0   0  | VB->VBN if Word:run@[0] & Word:*@[1] & Word:by@[2]\n",
      "   3   3   0   0  | DT->NN if Word:-LRB-@[-1] & Word:A@[0] & Word:-RRB-@[1]\n",
      "   3   3   0   0  | IN->RB if Word:,@[-1] & Word:though@[0] & Word:,@[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3   3   0   0  | IN->RB if Word:month@[-1] & Word:before@[0] & Word:.@[1]\n",
      "   3   3   0   0  | JJ->RB if Word:a@[-1] & Word:little@[0] & Word:more@[1]\n",
      "   3   3   0   0  | NN->JJ if Word:and@[-1] & Word:stock-index@[0] &\n",
      "                  |   Word:futures@[1]\n",
      "   3   3   0   0  | NNS->NNPS if Word:'s@[-1] & Word:Investors@[0] &\n",
      "                  |   Word:Service@[1]\n",
      "   3   4   1   1  | CD->NN if Word:one@[0] & Word:no@[-1]\n",
      "   3   3   0   0  | JJ->RB if Word:long@[0] & Word:have@[-1]\n",
      "   3   3   0   0  | NNP->JJ if Word:Western@[0] & Word:in@[-1]\n",
      "   3   3   0   0  | NNPS->NNP if Word:Industries@[0] & Word:Heavy@[-1]\n",
      "   3   3   0   0  | POS->VBZ if Word:'s@[0] & Word:that@[-1]\n",
      "   5   5   0   1  | IN->DT if Word:that@[0] & Pos:VBZ@[1]\n",
      "   3   3   0   0  | RB->RP if Word:away@[0] & Word:take@[-1]\n",
      "   3   3   0   0  | VBN->VBD if Word:proposed@[0] & Word:,@[-1]\n",
      "   3   3   0   0  | IN->RP if Word:off@[0] & Word:.@[1]\n",
      "   3   3   0   0  | JJ->IN if Word:outside@[0] & Word:the@[1]\n",
      "   3   3   0   0  | JJ->NN if Word:subject@[0] & Word:of@[1]\n",
      "   3   3   0   0  | JJ->RB if Word:overseas@[0] & Word:.@[1]\n",
      "   3   3   0   0  | NN->JJ if Word:executive@[0] & Word:power@[1]\n",
      "   3   3   0   1  | IN->DT if Word:that@[0] & Word:.@[2]\n",
      "   3   3   0   0  | NN->JJ if Word:stock-index@[0] & Word:S&P@[-2]\n",
      "   3   3   0   1  | RB->RP if Word:down@[0] & Word:to@[-2]\n",
      "   3   3   0   0  | VBD->VBN if Word:n't@[-2,-1]\n",
      "   3   3   0   0  | VBN->VBD if Word:yield@[-2,-1]\n",
      "   3   3   0   0  | POS->'' if Word:`@[-3,-2,-1]\n",
      "   3   3   0   0  | DT->PDT if Word:half@[0] & Pos:NN@[2]\n",
      "   3   3   0   0  | NN->NNS if Word:headquarters@[0] & Pos:NNP@[2]\n",
      "   3   3   0   0  | RB->IN if Word:so@[0] & Pos:NN@[2]\n",
      "   3   3   0   0  | NN->JJ if Word:close@[0] & Pos:VBG@[-2]\n",
      "   3   3   0   0  | NN->NNP if Word:Section@[0] & Pos:DT@[-2]\n",
      "   3   3   0   0  | VB->NN if Word:sell@[0] & Pos:NN@[-2]\n",
      "   3   3   0   0  | JJ->NN if Word:net@[0] & Pos:IN@[1]\n",
      "   3   3   0   0  | NN->IN if Word:worth@[0] & Pos:-NONE-@[1]\n",
      "   3   3   0   0  | NN->VBG if Word:offering@[0] & Pos:DT@[1]\n",
      "   3   3   0   0  | NNP->JJ if Word:New@[0] & Pos:NNS@[1]\n",
      "   3   3   0   0  | NNP->NN if Word:Stock@[0] & Pos:NNS@[1]\n",
      "   3   3   0   0  | RB->JJ if Word:enough@[0] & Pos:NN@[1]\n",
      "   3   4   1   1  | VB->VBN if Word:put@[0] & Pos:-NONE-@[1]\n",
      "   3   3   0   0  | VBG->NN if Word:working@[0] & Pos:NNS@[1]\n",
      "   3   3   0   0  | IN->RB if Word:up@[0] & Pos:VBD@[-1]\n",
      "   3   4   1   0  | JJ->IN if Word:next@[0] & Pos:NN@[-1]\n",
      "   3   3   0   0  | JJ->NN if Word:high@[0] & Pos:JJ@[-1]\n",
      "   3   3   0   0  | JJ->VBG if Word:closing@[0] & Pos:-NONE-@[-1]\n",
      "   3   3   0   0  | NN->JJ if Word:assistant@[0] & Pos:DT@[-1]\n",
      "   3   3   0   0  | NN->NNP if Word:Program@[0] & Pos:NNP@[-1]\n",
      "   3   3   0   0  | NN->RB if Word:right@[0] & Pos:NN@[-1]\n",
      "   3   3   0   0  | NN->VBP if Word:decline@[0] & Pos:NNS@[-1]\n",
      "   3   4   1   2  | RB->IN if Word:down@[0] & Pos:VBG@[-1]\n",
      "   3   3   0   0  | VB->NN if Word:pay@[0] & Pos:NN@[-1]\n",
      "   3   3   0   0  | VBG->NN if Word:buying@[0] & Pos:JJ@[-1]\n",
      "   3   5   2   3  | RB->RP if Word:off@[0]\n",
      "   4   4   0   2  | RP->RB if Word:,@[-1]\n",
      "   3   5   2   0  | NN->VBG if Word:are@[-1]\n",
      "   3  10   7   3  | VB->VBP if Word:,@[-1]\n",
      "   3   3   0   0  | JJ->PDT if Pos:VB@[-1] & Pos:DT@[1]\n",
      "   3   4   1   1  | VBN->VBD if Pos:,@[-1] & Pos:NNP@[1]\n",
      "   3   3   0   0  | VBN->VBD if Pos:NN@[-1] & Pos:NN@[1]\n",
      "   3   3   0   0  | VBN->VBD if Pos:NN@[-1] & Pos:PRP@[1]\n",
      "   3   4   1   0  | VBN->VBD if Pos:NNS@[-1] & Pos:IN@[1]\n",
      "   3   3   0   0  | NNPS->NNP if Pos:,@[1] & Pos:IN@[2]\n",
      "   3   3   0   0  | NN->VB if Pos:TO@[-1] & Pos:NNP@[-2]\n",
      "   3   3   0   0  | NNS->VBZ if Pos:RB@[-1] & Pos:NN@[-2]\n",
      "   3   3   0   0  | NNPS->NNS if Pos::@[-1]\n",
      "   3   3   0   0  | RP->IN if Pos:RB@[-1]\n",
      "   3   3   0   0  | VBZ->NNS if Pos:VBN@[-1]\n",
      "   3   3   0   0  | VBP->VB if Pos:MD@[-3,-2,-1]\n",
      "   3   3   0   0  | IN->RB if Pos:CD@[1] & Word:about@[0] & Word:has@[-1]\n",
      "   3   3   0   0  | NN->JJ if Pos:NN@[1] & Pos:-NONE-@[2] & Word:officer@[1]\n",
      "   2   2   0   0  | IN->NNP if Word:In@[0] & Word:the@[1] & Word:Dumpster@[2]\n",
      "   2   2   0   0  | JJ->NN if Word:minimum@[0] & Word:tax@[1] & Word:.@[2]\n",
      "   2   2   0   0  | JJ->RB if Word:early@[0] & Word:trading@[1] & Word:in@[2]\n",
      "   2   5   3   1  | JJR->RBR if Word:earlier@[0] & Word:this@[1] &\n",
      "                  |   Word:year@[2]\n",
      "   2   2   0   0  | JJR->RBR if Word:more@[0] & Word:than@[1] & Word:50@[2]\n",
      "   2   2   0   0  | NN->JJ if Word:stock-index@[0] & Word:arbitrage@[1] &\n",
      "                  |   Word:,@[2]\n",
      "   2   2   0   0  | NN->NNS if Word:yen@[0] & Word:late@[1] & Word:Tuesday@[2]\n",
      "   2   2   0   0  | NN->VBP if Word:account@[0] & Word:for@[1] &\n",
      "                  |   Word:almost@[2]\n",
      "   2   2   0   0  | NNP->NN if Word:Act@[0] & Word:shall@[1] & Word:be@[2]\n",
      "   2   2   0   0  | NNS->VBZ if Word:talks@[0] & Word:about@[1] & Word:the@[2]\n",
      "   2   2   0   0  | RB->IN if Word:down@[0] & Word:the@[1] & Word:road@[2]\n",
      "   2   2   0   0  | VB->VBN if Word:come@[0] & Word:to@[1] & Word:the@[2]\n",
      "   2   2   0   0  | VB->VBP if Word:give@[0] & Word:way@[1] & Word:to@[2]\n",
      "   2   2   0   0  | DT->NN if Word:nearly@[-1] & Word:half@[0] & Word:of@[1]\n",
      "   2   2   0   0  | IN->RB if Word:raise@[-1] & Word:about@[0] & Word:$@[1]\n",
      "   2   2   0   0  | IN->RB if Word:years@[-1] & Word:ago@[0] & Word:are@[1]\n",
      "   2   2   0   0  | JJ->IN if Word:bonds@[-1] & Word:next@[0] & Word:week@[1]\n",
      "   2   2   0   0  | JJ->NN if Word:in@[-1] & Word:excess@[0] & Word:of@[1]\n",
      "   2   2   0   0  | JJ->NN if Word:of@[-1] & Word:current@[0] & Word:that@[1]\n",
      "   2   2   0   0  | JJ->NN if Word:the@[-1] & Word:homeless@[0] & Word:,@[1]\n",
      "   2   2   0   0  | JJ->NN if Word:the@[-1] & Word:past@[0] & Word:,@[1]\n",
      "   2   2   0   0  | JJR->RBR if Word:fared@[-1] & Word:better@[0] &\n",
      "                  |   Word:than@[1]\n",
      "   2   2   0   0  | NN->JJ if Word:the@[-1] & Word:future@[0] & Word:growth@[1]\n",
      "   2   2   0   0  | NN->VBG if Word:moderate@[-1] & Word:trading@[0] &\n",
      "                  |   Word:.@[1]\n",
      "   2   3   1   0  | NN->VBG if Word:the@[-1] & Word:manufacturing@[0] &\n",
      "                  |   Word:sector@[1]\n",
      "   2   2   0   0  | NNP->JJ if Word:of@[-1] & Word:New@[0] & Word:York-\n",
      "                  |   based@[1]\n",
      "   2   2   0   0  | NNPS->NNP if Word:Foster@[-1] & Word:Savings@[0] &\n",
      "                  |   Word:Institution@[1]\n",
      "   2   2   0   0  | TO->IN if Word:--@[-1] & Word:to@[0] & Word:the@[1]\n",
      "   2   2   0   0  | VB->NN if Word:*U*@[-1] & Word:face@[0] & Word:amount@[1]\n",
      "   2   2   0   0  | VBG->NN if Word:level@[-1] & Word:playing@[0] &\n",
      "                  |   Word:field@[1]\n",
      "   2   2   0   1  | CD->NN if Word:one@[0] & Word:No@[-1]\n",
      "   2   2   0   0  | EX->RB if Word:there@[0] & Word:out@[-1]\n",
      "   2   2   0   0  | IN->RB if Word:ago@[0] & Word:long@[-1]\n",
      "   2   2   0   0  | IN->RP if Word:in@[0] & Word:lock@[-1]\n",
      "   2   2   0   0  | IN->RP if Word:in@[0] & Word:put@[-1]\n",
      "   2   2   0   0  | IN->RP if Word:in@[0] & Word:tune@[-1]\n",
      "   2   2   0   0  | JJ->VBP if Word:own@[0] & Word:they@[-1]\n",
      "   2   2   0   0  | NN->NNS if Word:yen@[0] & Word:10,000@[-1]\n",
      "   2   2   0   0  | NN->VB if Word:risk@[0] & Word:or@[-1]\n",
      "   2   2   0   0  | NN->VB if Word:veto@[0] & Word:to@[-1]\n",
      "   2   2   0   0  | NN->VBG if Word:financing@[0] & Word:home@[-1]\n",
      "   2   2   0   0  | NN->VBG if Word:manufacturing@[0] & Word:and@[-1]\n",
      "   2   2   0   0  | NNP->NN if Word:Midwest@[0] & Word:the@[-1]\n",
      "   2   2   0   0  | NNS->VBZ if Word:increases@[0] & Word:``@[-1]\n",
      "   2   2   0   0  | RB->CC if Word:yet@[0] & Word:,@[-1]\n"
     ]
    }
   ],
   "source": [
    "sentences = [tagger.tag(word_tokenize('Today is a beautiful morning'))]\n",
    "sentences = treebank.tagged_sents()\n",
    "templates = fntbl37()\n",
    "tagger = UnigramTagger(sentences)\n",
    "tagger = BrillTaggerTrainer(tagger, templates, trace=3)\n",
    "tagger = tagger.train(sentences, max_rules=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the trainer. ! There's a newer way to save models.\n",
    "import pickle\n",
    "with open('demo.pkl', 'wb') as f:\n",
    "    pickle.dump(tagger, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical modelling involving n-gram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = treebank.tagged_sents()[:7000]\n",
    "# Perform training using the first 7000 sentences of the treebank corpus.\n",
    "tagger = UnigramTagger(training)\n",
    "treebank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619024159944167"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import UnigramTagger\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "\n",
    "# Evaluating the tagging.\n",
    "tagger = UnigramTagger(training)\n",
    "# We obtain an accuracy of 96%\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7972986842375351"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a cutoff frequency of 5.\n",
    "tagger = UnigramTagger(training, cutoff=5)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619024159944167"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tag = DefaultTagger('NN')\n",
    "tagger = UnigramTagger(training, backoff=tag)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9171131227292321"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BigramTagger make use of the previous tag as contextual information.\n",
    "# TrigramTagger make use of the previous two tags as contextual information.\n",
    "\n",
    "from nltk.tag import BigramTagger\n",
    "tagger = BigramTagger(training)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9022107272615308"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import TrigramTagger\n",
    "tagger = TrigramTagger(training)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9304554878173943"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import NgramTagger\n",
    "tagger = NgramTagger(4, training)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2902682841718497"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AffixTagger is also a ContextTagger which makes use of a prefix or suffix as the contextual information.\n",
    "from nltk.tag import AffixTagger\n",
    "tagger = AffixTagger(training)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2094751318841472"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code make use of four character prefix.\n",
    "tagger = AffixTagger(training, affix_length=4)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2902682841718497"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code make use of three character suffix.\n",
    "tagger = AffixTagger(training, affix_length=-3)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29166410082722666"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combination of many affix tagger.\n",
    "tagger1 = AffixTagger(training, affix_length = 4)\n",
    "tagger2 = AffixTagger(training, affix_length = -3, backoff=tagger1)\n",
    "tagger2.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TnT is Trigrams n Tags, a statistical-based tagger that is based on the second order Markov models.\n",
    "from nltk.tag import tnt\n",
    "from nltk.corpus import treebank\n",
    "tagger = tnt.TnT()\n",
    "tagger.train(training)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set a default for unknown tagger.\n",
    "unknown = DefaultTagger('NN')\n",
    "tagger = tnt.TnT(unk=unknown, Trained=True)\n",
    "tagger.train(training)\n",
    "tagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a chunker using pos-tagger corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('wise', 'NN'),\n",
       " ('small', 'JJ'),\n",
       " ('girl', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('village', 'NN'),\n",
       " ('became', 'VBD'),\n",
       " ('leader', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "text = 'A wise small girl of village became leader'\n",
    "sentence = pos_tag(word_tokenize(text))\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/var/folders/pt/v3mw_j891dv8yl5k2b5p54wm0000gn/T/tmpzfgz1tpo.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/ipython/7.5.0/libexec/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m                 ).split()\n\u001b[1;32m    803\u001b[0m             )\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/var/folders/pt/v3mw_j891dv8yl5k2b5p54wm0000gn/T/tmpzfgz1tpo.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('A', 'DT'), ('wise', 'NN')]), Tree('NP', [('small', 'JJ'), ('girl', 'NN'), ('of', 'IN'), ('village', 'NN')]), ('became', 'VBD'), Tree('NP', [('leader', 'NN')])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN><IN>?<NN>*}\"\n",
    "find = RegexpParser(grammar)\n",
    "res = find.parse(sentence)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
