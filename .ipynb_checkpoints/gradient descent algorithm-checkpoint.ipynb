{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "- iterative optimization algorithm for finding the minimum of a function\n",
    "- the algorithm computes the slope (gradient) that is first order derivative of the function at the current point\n",
    "- move in the opposite direction of the slope increase from the current point by the computed amount\n",
    "\n",
    "\n",
    "## Types of Gradient Descent Algorithms\n",
    "\n",
    "- batch gradient descent\n",
    "- stochastic gradient descent\n",
    "- mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "Similar to stochastic gradient descent. But instead of updating the paramaters of the network after computing the loss of every training sample in the training set, the parameters are updated once that is after all the training examples have been passed through the network. \n",
    "\n",
    "Pros:\n",
    "- less oscillations and noisy steps taken towards the global minima of the loss function due to updating the parameters by computing the average of all the training samples rather than the values of a single step\n",
    "- it can benefit from the vectorization which increases the speed of processing all training samples together\n",
    "- it produces a more stable gradient descent convergence and stable error gradient that stochastic gradient descent\n",
    "- computationally more efficient as all the resources are not being used to process a single sampe rather are being used for all training samples\n",
    "\n",
    "Cons:\n",
    "- can lead to local minima and unlike stochastic gradient descent, no noisy steps are there to help get out of the local minima\n",
    "- the entire training set can be too large to process in the memory due to which additional memory might be needed\n",
    "- may take longer for processing all the samples as batch\n",
    "\n",
    "\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = y.size # Number of training examples.\n",
    "    for i in range(num_iters):\n",
    "        y_hat = np.dot(X, theta)\n",
    "        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat - y)\n",
    "    return theta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Pros:\n",
    "- it is easier to fit into memory due to a single training being processed by the network\n",
    "- computationally fast as only one sample is processed at a time\n",
    "- converges faster for larger dataset as it causes updates to the paramater more frequently\n",
    "\n",
    "Cons:\n",
    "- can be noisy due to frequent updates, and also lead the gradient descent into other directions\n",
    "- may take longer to achieve convergence to the minima of the loss function\n",
    "- loses the advantage of vectorized operations as it deals with only a single example at a time\n",
    "\n",
    "\n",
    "Code example:\n",
    "\n",
    "```python\n",
    "def SGD(cost_fn, theta0, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    cost_fn -- the function to optimize, takes a single argument and yields the cost and gradient\n",
    "    theta0 -- the initial point to start SGD from\n",
    "    num_iters -- total iteration to run SGD\n",
    "    Return:\n",
    "    theta -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "    start_iter = 0\n",
    "    theta = theta0\n",
    "    for _ in range(start_iter + 1, num_iters + 1):\n",
    "        _, gradient = cost_fn(theta)\n",
    "        theta = theta - (alpha * gradient) # There is NO dot product.\n",
    "    return theta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Gradient Descent: A compromise\n",
    "\n",
    "This is a mixture of both stochastic and batch gradient descent. The training set is divided into multiple groups called batches. Each batch has a number of training samples in it.\n",
    "\n",
    "At a time a single batch is passed through the network which computes the loss of every sample in the batch and uses their average to update the parameters of the neural network. \n",
    "\n",
    "For example, say the training set has 100 training examples which is divided into 5 batches with each batch containing 20 training examples. This means that the equation will be iterated over 5 times (the number of batches).\n",
    "\n",
    "Pros:\n",
    "- Easily fits into memory\n",
    "- computationally efficient\n",
    "- benefit from vectorization\n",
    "- if it stucks in local minimum, some noisy steps can lead the way out of them\n",
    "- average of the training samples produces stable error gradients and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"Computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta y / delta x\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10d415470>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(-10, 10)\n",
    "plt.title('Actual Derivatives vs. Estimates')\n",
    "plt.plot(x, list(map(derivative, x)), 'rx', label='Actual')\n",
    "plt.plot(x, list(map(derivative_estimate, x)), 'b+', label='Estimate')\n",
    "plt.legend(loc=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"Compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0) # Add h to the ith element of v\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"Move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i \n",
    "            for v_i, direction_i in zip(v, direction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def vector_subtract(v, w):\n",
    "    return [v_i - w_i\n",
    "            for v_i, w_i in zip(v, w)]\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def dot(v, w):\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    return dot(v, v)\n",
    "\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def distance(v, w):\n",
    "    return magnitude(vector_subtract(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Pick a random starting point.\n",
    "v = [random.randint(-10, 10) for i in range(3)]\n",
    "tolerance = 0.0000001\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v) # Compute the gradient at v.\n",
    "    next_v = step(v, gradient, -0.01) # Take a negative gradient step.\n",
    "    if distance(next_v, v) < tolerance: # Stop if we are converging.\n",
    "        break\n",
    "    v = next_v # Continue if we are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5964400041392797e-06, 2.39466000620892e-06, 3.991100010348203e-06]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right step size\n",
    "\n",
    "- using a fixed step size\n",
    "- gradually shrinking the step size over time\n",
    "- at each step, choosing the step size that minimizes the value of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"Return a new function that's the same as f, except that it outputs infinity whenever f produces an error\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"Use gradient descent to find theta that minimizes target function.\"\"\"\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.000001]\n",
    "    \n",
    "    theta = theta_0 # Set theta to initial value.\n",
    "    target_fn = safe(target_fn)\n",
    "    value = target_fn(theta)\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size) \n",
    "                       for step_size in step_sizes]\n",
    "        \n",
    "        # Choose the one that minimizes the error function.\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # Stop if we are converging.\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"Return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_all(f):\n",
    "    \"\"\"The same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn), \n",
    "                          theta_0,\n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"Generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)] \n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha=0.01):\n",
    "    data = zip(x, y)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    min_theta, min_value = None, float('inf')\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # If we ever go 100 iterations with no improvements, stop.\n",
    "    while iterations_with_no_improvements < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # If we found a new minimum, remember it and go back \n",
    "            # to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvements = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # Otherwise we are not improving, so try shrinking the step size.\n",
    "            iterations_with_no_improvements += 1\n",
    "            alpha *= 0.9\n",
    "        \n",
    "        # And take a gradient step for each of the data points.\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-13cc8a07d8e6>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-13cc8a07d8e6>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    negate_all(gradient_fn),\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn)\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Stochastic Gradient Descent From Scratch\n",
    "\n",
    "https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "with open('./data/wine_quality/winequality-white.csv') as f:\n",
    "    header = f.readline()\n",
    "    for col in header.split(';'):  # Split by \";\"\n",
    "        col = col.strip()          # Remove new lines.\n",
    "        col = col.replace('\"', '') # Remove quotations.\n",
    "        col = col.split(' ')       # Split by whitespace.\n",
    "        col = '_'.join(col)        # Join by underscore.\n",
    "        labels.append(col)         # Append to the labels.\n",
    "\n",
    "    Wine = namedtuple('Wine', labels) # Create a new namedtuple from the labels.\n",
    "    for line in f.readlines():\n",
    "        row = line.split(';')\n",
    "        row = [float(col.strip()) \n",
    "               for col in row] # Convert cols to float.\n",
    "        wine = Wine._make(row)\n",
    "        data.append(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed_acidity',\n",
       " 'volatile_acidity',\n",
       " 'citric_acid',\n",
       " 'residual_sugar',\n",
       " 'chlorides',\n",
       " 'free_sulfur_dioxide',\n",
       " 'total_sulfur_dioxide',\n",
       " 'density',\n",
       " 'pH',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'quality']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wine(fixed_acidity=7.0, volatile_acidity=0.27, citric_acid=0.36, residual_sugar=20.7, chlorides=0.045, free_sulfur_dioxide=45.0, total_sulfur_dioxide=170.0, density=1.001, pH=3.0, sulphates=0.45, alcohol=8.8, quality=6.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8, 6.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, coefficients):\n",
    "    # Make predictions with coefficients.\n",
    "    y_hat = coefficients[0]\n",
    "    for i in range(len(row) - 1):\n",
    "        y_hat += coefficients[i + 1] * row[i]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected 1.000, predicted: 1.200\n",
      "expected 3.000, predicted: 2.000\n",
      "expected 3.000, predicted: 3.600\n",
      "expected 2.000, predicted: 2.800\n",
      "expected 5.000, predicted: 4.400\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "coef = [0.4, 0.8]\n",
    "\n",
    "for row in dataset:\n",
    "    y_hat = predict(row, coef)\n",
    "    print(f'expected {row[-1]:.3f}, predicted: {y_hat:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate linear regression coefficients using stochastic gradient descent.\n",
    "\n",
    "# https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings\n",
    "\n",
    "def stochastic_gradient_descent(train, a, epoch):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : list str\n",
    "        The training dataset\n",
    "    a : int\n",
    "        The learning rate\n",
    "    epoch : int\n",
    "        The number of iterations to train the data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    coef : list int\n",
    "        The coefficient\n",
    "    \"\"\"\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for i in range(epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            y_hat = predict(row, coef) # Predict the y_i value.\n",
    "            error = y_hat - row[-1]    # Error is the difference between predicted and actual value.\n",
    "            sum_error += error ** 2    # Sum square error.\n",
    "            coef[0] = coef[0] - a * error\n",
    "            for j in range(len(row) - 1):\n",
    "                coef[j + 1] = coef[j + 1] - a * error * row[j]\n",
    "        print(f'>epoch={i} a={a:.3f} error={sum_error:.3f}')\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0 a=0.001 error=46.236\n",
      ">epoch=1 a=0.001 error=41.305\n",
      ">epoch=2 a=0.001 error=36.930\n",
      ">epoch=3 a=0.001 error=33.047\n",
      ">epoch=4 a=0.001 error=29.601\n",
      ">epoch=5 a=0.001 error=26.543\n",
      ">epoch=6 a=0.001 error=23.830\n",
      ">epoch=7 a=0.001 error=21.422\n",
      ">epoch=8 a=0.001 error=19.285\n",
      ">epoch=9 a=0.001 error=17.389\n",
      ">epoch=10 a=0.001 error=15.706\n",
      ">epoch=11 a=0.001 error=14.213\n",
      ">epoch=12 a=0.001 error=12.888\n",
      ">epoch=13 a=0.001 error=11.712\n",
      ">epoch=14 a=0.001 error=10.668\n",
      ">epoch=15 a=0.001 error=9.742\n",
      ">epoch=16 a=0.001 error=8.921\n",
      ">epoch=17 a=0.001 error=8.191\n",
      ">epoch=18 a=0.001 error=7.544\n",
      ">epoch=19 a=0.001 error=6.970\n",
      ">epoch=20 a=0.001 error=6.461\n",
      ">epoch=21 a=0.001 error=6.009\n",
      ">epoch=22 a=0.001 error=5.607\n",
      ">epoch=23 a=0.001 error=5.251\n",
      ">epoch=24 a=0.001 error=4.935\n",
      ">epoch=25 a=0.001 error=4.655\n",
      ">epoch=26 a=0.001 error=4.406\n",
      ">epoch=27 a=0.001 error=4.186\n",
      ">epoch=28 a=0.001 error=3.990\n",
      ">epoch=29 a=0.001 error=3.816\n",
      ">epoch=30 a=0.001 error=3.662\n",
      ">epoch=31 a=0.001 error=3.525\n",
      ">epoch=32 a=0.001 error=3.404\n",
      ">epoch=33 a=0.001 error=3.296\n",
      ">epoch=34 a=0.001 error=3.200\n",
      ">epoch=35 a=0.001 error=3.115\n",
      ">epoch=36 a=0.001 error=3.040\n",
      ">epoch=37 a=0.001 error=2.973\n",
      ">epoch=38 a=0.001 error=2.914\n",
      ">epoch=39 a=0.001 error=2.862\n",
      ">epoch=40 a=0.001 error=2.815\n",
      ">epoch=41 a=0.001 error=2.773\n",
      ">epoch=42 a=0.001 error=2.737\n",
      ">epoch=43 a=0.001 error=2.704\n",
      ">epoch=44 a=0.001 error=2.675\n",
      ">epoch=45 a=0.001 error=2.650\n",
      ">epoch=46 a=0.001 error=2.627\n",
      ">epoch=47 a=0.001 error=2.607\n",
      ">epoch=48 a=0.001 error=2.589\n",
      ">epoch=49 a=0.001 error=2.573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22998234937311363, 0.8017220304137576]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate coefficients.\n",
    "a = 0.001\n",
    "epoch = 50\n",
    "coef = stochastic_gradient_descent(dataset, a, epoch)\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/\n",
    "def min_max(data):\n",
    "    \"\"\"Find the min and max values for each columns.\"\"\"\n",
    "    minmax = []\n",
    "    cols = len(data[0])\n",
    "    for i in range(cols):\n",
    "        values = [row[i] for row in data] # The column values.\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        minmax.append((min_val, max_val))\n",
    "    return minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, minmax):\n",
    "    \"\"\"Rescale columns to the range 0-1.\"\"\"\n",
    "    normalized = []\n",
    "    for row in data:\n",
    "        cols = []\n",
    "        for j in range(len(row)):\n",
    "            num = row[j] - minmax[j][0]\n",
    "            den = minmax[j][1] - minmax[j][0]\n",
    "            cols.append(num / den) \n",
    "        normalized.append(Wine._make(cols))\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def cross_validation_split(data, n_folds):\n",
    "    \"\"\"Split a dataset into k folds.\"\"\"\n",
    "    folds = []\n",
    "    copy = data.copy() # Make a copy so that the original list is not affected.\n",
    "    fold_size = len(data) // n_folds\n",
    "    for i in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = random.randrange(len(copy)) # Select a random row.\n",
    "            fold.append(copy.pop(index)) # Add to the current fold (bucket).\n",
    "        folds.append(fold) # Add the fold to the list of folds.\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def root_mean_squared_error(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return math.sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(data, algorithm, n_folds, *args):\n",
    "    \"\"\"Evaluate an algorithm using a cross validation split.\"\"\"\n",
    "    folds = cross_validation_split(data, n_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train = folds.copy()\n",
    "        train.remove(fold)\n",
    "        train = sum(train, [])\n",
    "        test = []\n",
    "        for row in fold:\n",
    "            test.append(row)\n",
    "        predicted = algorithm(train, test, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        rmse = root_mean_squared_error(actual, predicted)\n",
    "        scores.append(rmse)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_sgd(train, test, a, epoch):\n",
    "    predictions = []\n",
    "    coef = stochastic_gradient_descent(train, a, epoch)\n",
    "    for row in test:\n",
    "        y_hat = predict(row, coef)\n",
    "        predictions.append(y_hat)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0 a=0.010 error=79.062\n",
      ">epoch=1 a=0.010 error=67.512\n",
      ">epoch=2 a=0.010 error=65.919\n",
      ">epoch=3 a=0.010 error=65.037\n",
      ">epoch=4 a=0.010 error=64.511\n",
      ">epoch=5 a=0.010 error=64.185\n",
      ">epoch=6 a=0.010 error=63.975\n",
      ">epoch=7 a=0.010 error=63.836\n",
      ">epoch=8 a=0.010 error=63.742\n",
      ">epoch=9 a=0.010 error=63.676\n",
      ">epoch=10 a=0.010 error=63.628\n",
      ">epoch=11 a=0.010 error=63.593\n",
      ">epoch=12 a=0.010 error=63.566\n",
      ">epoch=13 a=0.010 error=63.544\n",
      ">epoch=14 a=0.010 error=63.527\n",
      ">epoch=15 a=0.010 error=63.512\n",
      ">epoch=16 a=0.010 error=63.500\n",
      ">epoch=17 a=0.010 error=63.489\n",
      ">epoch=18 a=0.010 error=63.479\n",
      ">epoch=19 a=0.010 error=63.469\n",
      ">epoch=20 a=0.010 error=63.461\n",
      ">epoch=21 a=0.010 error=63.452\n",
      ">epoch=22 a=0.010 error=63.444\n",
      ">epoch=23 a=0.010 error=63.437\n",
      ">epoch=24 a=0.010 error=63.430\n",
      ">epoch=25 a=0.010 error=63.423\n",
      ">epoch=26 a=0.010 error=63.416\n",
      ">epoch=27 a=0.010 error=63.409\n",
      ">epoch=28 a=0.010 error=63.402\n",
      ">epoch=29 a=0.010 error=63.396\n",
      ">epoch=30 a=0.010 error=63.390\n",
      ">epoch=31 a=0.010 error=63.383\n",
      ">epoch=32 a=0.010 error=63.377\n",
      ">epoch=33 a=0.010 error=63.371\n",
      ">epoch=34 a=0.010 error=63.365\n",
      ">epoch=35 a=0.010 error=63.359\n",
      ">epoch=36 a=0.010 error=63.353\n",
      ">epoch=37 a=0.010 error=63.348\n",
      ">epoch=38 a=0.010 error=63.342\n",
      ">epoch=39 a=0.010 error=63.336\n",
      ">epoch=40 a=0.010 error=63.331\n",
      ">epoch=41 a=0.010 error=63.325\n",
      ">epoch=42 a=0.010 error=63.320\n",
      ">epoch=43 a=0.010 error=63.314\n",
      ">epoch=44 a=0.010 error=63.309\n",
      ">epoch=45 a=0.010 error=63.304\n",
      ">epoch=46 a=0.010 error=63.298\n",
      ">epoch=47 a=0.010 error=63.293\n",
      ">epoch=48 a=0.010 error=63.288\n",
      ">epoch=49 a=0.010 error=63.283\n",
      ">epoch=0 a=0.010 error=78.440\n",
      ">epoch=1 a=0.010 error=65.663\n",
      ">epoch=2 a=0.010 error=64.090\n",
      ">epoch=3 a=0.010 error=63.227\n",
      ">epoch=4 a=0.010 error=62.717\n",
      ">epoch=5 a=0.010 error=62.404\n",
      ">epoch=6 a=0.010 error=62.205\n",
      ">epoch=7 a=0.010 error=62.074\n",
      ">epoch=8 a=0.010 error=61.986\n",
      ">epoch=9 a=0.010 error=61.924\n",
      ">epoch=10 a=0.010 error=61.880\n",
      ">epoch=11 a=0.010 error=61.848\n",
      ">epoch=12 a=0.010 error=61.823\n",
      ">epoch=13 a=0.010 error=61.804\n",
      ">epoch=14 a=0.010 error=61.788\n",
      ">epoch=15 a=0.010 error=61.775\n",
      ">epoch=16 a=0.010 error=61.763\n",
      ">epoch=17 a=0.010 error=61.753\n",
      ">epoch=18 a=0.010 error=61.744\n",
      ">epoch=19 a=0.010 error=61.735\n",
      ">epoch=20 a=0.010 error=61.728\n",
      ">epoch=21 a=0.010 error=61.720\n",
      ">epoch=22 a=0.010 error=61.713\n",
      ">epoch=23 a=0.010 error=61.706\n",
      ">epoch=24 a=0.010 error=61.699\n",
      ">epoch=25 a=0.010 error=61.692\n",
      ">epoch=26 a=0.010 error=61.686\n",
      ">epoch=27 a=0.010 error=61.679\n",
      ">epoch=28 a=0.010 error=61.673\n",
      ">epoch=29 a=0.010 error=61.667\n",
      ">epoch=30 a=0.010 error=61.661\n",
      ">epoch=31 a=0.010 error=61.655\n",
      ">epoch=32 a=0.010 error=61.649\n",
      ">epoch=33 a=0.010 error=61.643\n",
      ">epoch=34 a=0.010 error=61.637\n",
      ">epoch=35 a=0.010 error=61.631\n",
      ">epoch=36 a=0.010 error=61.626\n",
      ">epoch=37 a=0.010 error=61.620\n",
      ">epoch=38 a=0.010 error=61.614\n",
      ">epoch=39 a=0.010 error=61.609\n",
      ">epoch=40 a=0.010 error=61.603\n",
      ">epoch=41 a=0.010 error=61.597\n",
      ">epoch=42 a=0.010 error=61.592\n",
      ">epoch=43 a=0.010 error=61.586\n",
      ">epoch=44 a=0.010 error=61.581\n",
      ">epoch=45 a=0.010 error=61.576\n",
      ">epoch=46 a=0.010 error=61.570\n",
      ">epoch=47 a=0.010 error=61.565\n",
      ">epoch=48 a=0.010 error=61.560\n",
      ">epoch=49 a=0.010 error=61.554\n",
      ">epoch=0 a=0.010 error=79.834\n",
      ">epoch=1 a=0.010 error=66.690\n",
      ">epoch=2 a=0.010 error=64.987\n",
      ">epoch=3 a=0.010 error=64.069\n",
      ">epoch=4 a=0.010 error=63.535\n",
      ">epoch=5 a=0.010 error=63.211\n",
      ">epoch=6 a=0.010 error=63.008\n",
      ">epoch=7 a=0.010 error=62.877\n",
      ">epoch=8 a=0.010 error=62.790\n",
      ">epoch=9 a=0.010 error=62.731\n",
      ">epoch=10 a=0.010 error=62.690\n",
      ">epoch=11 a=0.010 error=62.660\n",
      ">epoch=12 a=0.010 error=62.638\n",
      ">epoch=13 a=0.010 error=62.622\n",
      ">epoch=14 a=0.010 error=62.609\n",
      ">epoch=15 a=0.010 error=62.598\n",
      ">epoch=16 a=0.010 error=62.590\n",
      ">epoch=17 a=0.010 error=62.582\n",
      ">epoch=18 a=0.010 error=62.575\n",
      ">epoch=19 a=0.010 error=62.569\n",
      ">epoch=20 a=0.010 error=62.563\n",
      ">epoch=21 a=0.010 error=62.558\n",
      ">epoch=22 a=0.010 error=62.552\n",
      ">epoch=23 a=0.010 error=62.547\n",
      ">epoch=24 a=0.010 error=62.543\n",
      ">epoch=25 a=0.010 error=62.538\n",
      ">epoch=26 a=0.010 error=62.533\n",
      ">epoch=27 a=0.010 error=62.529\n",
      ">epoch=28 a=0.010 error=62.524\n",
      ">epoch=29 a=0.010 error=62.520\n",
      ">epoch=30 a=0.010 error=62.516\n",
      ">epoch=31 a=0.010 error=62.511\n",
      ">epoch=32 a=0.010 error=62.507\n",
      ">epoch=33 a=0.010 error=62.503\n",
      ">epoch=34 a=0.010 error=62.499\n",
      ">epoch=35 a=0.010 error=62.495\n",
      ">epoch=36 a=0.010 error=62.491\n",
      ">epoch=37 a=0.010 error=62.487\n",
      ">epoch=38 a=0.010 error=62.483\n",
      ">epoch=39 a=0.010 error=62.479\n",
      ">epoch=40 a=0.010 error=62.475\n",
      ">epoch=41 a=0.010 error=62.471\n",
      ">epoch=42 a=0.010 error=62.467\n",
      ">epoch=43 a=0.010 error=62.463\n",
      ">epoch=44 a=0.010 error=62.460\n",
      ">epoch=45 a=0.010 error=62.456\n",
      ">epoch=46 a=0.010 error=62.452\n",
      ">epoch=47 a=0.010 error=62.448\n",
      ">epoch=48 a=0.010 error=62.445\n",
      ">epoch=49 a=0.010 error=62.441\n",
      ">epoch=0 a=0.010 error=79.207\n",
      ">epoch=1 a=0.010 error=66.076\n",
      ">epoch=2 a=0.010 error=64.391\n",
      ">epoch=3 a=0.010 error=63.483\n",
      ">epoch=4 a=0.010 error=62.954\n",
      ">epoch=5 a=0.010 error=62.634\n",
      ">epoch=6 a=0.010 error=62.434\n",
      ">epoch=7 a=0.010 error=62.304\n",
      ">epoch=8 a=0.010 error=62.217\n",
      ">epoch=9 a=0.010 error=62.157\n",
      ">epoch=10 a=0.010 error=62.115\n",
      ">epoch=11 a=0.010 error=62.084\n",
      ">epoch=12 a=0.010 error=62.061\n",
      ">epoch=13 a=0.010 error=62.044\n",
      ">epoch=14 a=0.010 error=62.030\n",
      ">epoch=15 a=0.010 error=62.018\n",
      ">epoch=16 a=0.010 error=62.008\n",
      ">epoch=17 a=0.010 error=62.000\n",
      ">epoch=18 a=0.010 error=61.992\n",
      ">epoch=19 a=0.010 error=61.985\n",
      ">epoch=20 a=0.010 error=61.978\n",
      ">epoch=21 a=0.010 error=61.972\n",
      ">epoch=22 a=0.010 error=61.966\n",
      ">epoch=23 a=0.010 error=61.961\n",
      ">epoch=24 a=0.010 error=61.956\n",
      ">epoch=25 a=0.010 error=61.950\n",
      ">epoch=26 a=0.010 error=61.945\n",
      ">epoch=27 a=0.010 error=61.940\n",
      ">epoch=28 a=0.010 error=61.935\n",
      ">epoch=29 a=0.010 error=61.931\n",
      ">epoch=30 a=0.010 error=61.926\n",
      ">epoch=31 a=0.010 error=61.921\n",
      ">epoch=32 a=0.010 error=61.917\n",
      ">epoch=33 a=0.010 error=61.912\n",
      ">epoch=34 a=0.010 error=61.908\n",
      ">epoch=35 a=0.010 error=61.903\n",
      ">epoch=36 a=0.010 error=61.899\n",
      ">epoch=37 a=0.010 error=61.895\n",
      ">epoch=38 a=0.010 error=61.890\n",
      ">epoch=39 a=0.010 error=61.886\n",
      ">epoch=40 a=0.010 error=61.882\n",
      ">epoch=41 a=0.010 error=61.878\n",
      ">epoch=42 a=0.010 error=61.874\n",
      ">epoch=43 a=0.010 error=61.870\n",
      ">epoch=44 a=0.010 error=61.866\n",
      ">epoch=45 a=0.010 error=61.862\n",
      ">epoch=46 a=0.010 error=61.858\n",
      ">epoch=47 a=0.010 error=61.854\n",
      ">epoch=48 a=0.010 error=61.850\n",
      ">epoch=49 a=0.010 error=61.846\n",
      ">epoch=0 a=0.010 error=80.522\n",
      ">epoch=1 a=0.010 error=67.386\n",
      ">epoch=2 a=0.010 error=65.629\n",
      ">epoch=3 a=0.010 error=64.670\n",
      ">epoch=4 a=0.010 error=64.107\n",
      ">epoch=5 a=0.010 error=63.765\n",
      ">epoch=6 a=0.010 error=63.549\n",
      ">epoch=7 a=0.010 error=63.409\n",
      ">epoch=8 a=0.010 error=63.315\n",
      ">epoch=9 a=0.010 error=63.250\n",
      ">epoch=10 a=0.010 error=63.204\n",
      ">epoch=11 a=0.010 error=63.170\n",
      ">epoch=12 a=0.010 error=63.144\n",
      ">epoch=13 a=0.010 error=63.124\n",
      ">epoch=14 a=0.010 error=63.107\n",
      ">epoch=15 a=0.010 error=63.094\n",
      ">epoch=16 a=0.010 error=63.082\n",
      ">epoch=17 a=0.010 error=63.072\n",
      ">epoch=18 a=0.010 error=63.063\n",
      ">epoch=19 a=0.010 error=63.054\n",
      ">epoch=20 a=0.010 error=63.046\n",
      ">epoch=21 a=0.010 error=63.039\n",
      ">epoch=22 a=0.010 error=63.032\n",
      ">epoch=23 a=0.010 error=63.026\n",
      ">epoch=24 a=0.010 error=63.020\n",
      ">epoch=25 a=0.010 error=63.014\n",
      ">epoch=26 a=0.010 error=63.008\n",
      ">epoch=27 a=0.010 error=63.002\n",
      ">epoch=28 a=0.010 error=62.997\n",
      ">epoch=29 a=0.010 error=62.991\n",
      ">epoch=30 a=0.010 error=62.986\n",
      ">epoch=31 a=0.010 error=62.981\n",
      ">epoch=32 a=0.010 error=62.976\n",
      ">epoch=33 a=0.010 error=62.971\n",
      ">epoch=34 a=0.010 error=62.966\n",
      ">epoch=35 a=0.010 error=62.961\n",
      ">epoch=36 a=0.010 error=62.957\n",
      ">epoch=37 a=0.010 error=62.952\n",
      ">epoch=38 a=0.010 error=62.948\n",
      ">epoch=39 a=0.010 error=62.943\n",
      ">epoch=40 a=0.010 error=62.939\n",
      ">epoch=41 a=0.010 error=62.934\n",
      ">epoch=42 a=0.010 error=62.930\n",
      ">epoch=43 a=0.010 error=62.926\n",
      ">epoch=44 a=0.010 error=62.921\n",
      ">epoch=45 a=0.010 error=62.917\n",
      ">epoch=46 a=0.010 error=62.913\n",
      ">epoch=47 a=0.010 error=62.909\n",
      ">epoch=48 a=0.010 error=62.905\n",
      ">epoch=49 a=0.010 error=62.901\n"
     ]
    }
   ],
   "source": [
    "mm = min_max(data)\n",
    "nn = normalize(data, mm)\n",
    "n_folds = 5 # Folds.\n",
    "a = 0.01    # Learning rate.\n",
    "epoch = 50  # Epoch.\n",
    "\n",
    "scores = evaluate_algorithm(nn, linear_regression_sgd, n_folds, a, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('scores',\n",
       " [0.12248058224159092,\n",
       "  0.13034017509167112,\n",
       "  0.12620370547483578,\n",
       "  0.12897687952843237,\n",
       "  0.12446990678682233])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'scores', scores # ('scores', [0.12248058224159092, 0.13034017509167112, 0.12620370547483578, 0.12897687952843237, 0.12446990678682233])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mean rmse', '0.126')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'mean rmse', f'{sum(scores)/len(scores):.3f}' # ('mean rmse', '0.126')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
