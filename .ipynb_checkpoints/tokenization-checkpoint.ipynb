{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of text into sentences\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html?highlight=regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "# Split text into sentences with sent_tokenize.\n",
    "text = \"Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'Hope all are fine and doing well.',\n",
       " 'Hope you find the book interesting']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To tokenize large amount of sentences, we can load PunktSentenceTokenizer and use the tokenize function.\n",
    "import nltk\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = \"Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of text in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hallo!', 'Guten Tag!', 'Wie geht es dir?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "german_tokenizer.tokenize('Hallo! Guten Tag! Wie geht es dir?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of sentences into word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinkin',\n",
       " ',',\n",
       " '59',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'on',\n",
       " 'Nov.',\n",
       " '29']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "word_tokenize('Pierre Vinkin, 59 years old, will join as a nonexecutive director on Nov. 29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using TreeBankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'day',\n",
       " '!',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'the',\n",
       " 'book',\n",
       " 'interesting',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('Have a nice day! I hope you find the book interesting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tokenizer works by separating contractions.\n",
    "nltk.word_tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPunctTokenizer works by splitting punctuation.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of instantiating a class, an alternative way is to use the function.\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"Don't hesitate to ask questions\", '\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize using whitespace\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting words that starts with a capital.\n",
    "tokenizer = RegexpTokenizer('[A-Z]\\w+')\n",
    "tokenizer.tokenize(\"She secured 90.56% in class X. She is a meritorious student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She secured 90.56% in class X. \\nShe is a meritorious student.\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using predefined regular expression.\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "tokenizer = BlanklineTokenizer()\n",
    "tokenizer.tokenize(\"\"\"She secured 90.56% in class X. \n",
    "She is a meritorious student.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'secured',\n",
       " '90.56%',\n",
       " 'in',\n",
       " 'class',\n",
       " 'X.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meritorious',\n",
       " 'student.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenizer.tokenize(\"She secured 90.56% in class X. She is a meritorious student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
